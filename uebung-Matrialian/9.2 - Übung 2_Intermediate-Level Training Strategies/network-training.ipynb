{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **KogSys-ML-B: Einführung in Maschinelles Lernen**\n",
    "## **Deep Learning 2: Training Procedure**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up a new conda environment suitable for this notebook, you can use the following console commands:\n",
    "\n",
    "```bash\n",
    "conda create -y -n pytorch python=3.13\n",
    "conda activate pytorch\n",
    "python -m pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "**Note**: Conda can become very hard-drive hungry when you use many environments. Consider regularly deleting environments you no longer need and running the ``conda clean --all`` command to remove no longer needed packages and cached files.\n",
    "\n",
    "You can also install the requirements for this notebook into an existing environment by running the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install -q -U -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.manual_seed(2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1 Recap**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In last week's tutorial, we have seen the basic functionality of PyTorch and implemented our first own CNN! A possible solution for this is implemented here, so if you still want to catch up on last weeks exercises, don't look too closely at the cells from this section!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### choosing the device dynamically ###\n",
    "\n",
    "\n",
    "def get_device() -> str:\n",
    "    \"\"\"\n",
    "    Automatically checks if PyTorch has been installed for the use with CUDA (on NVIDIA GPUs) or MPS (Metal Performance Shaders, on Apple M chips). If neither is available, the CPU is used.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        \"cuda\"\n",
    "        if torch.cuda.is_available()\n",
    "        else \"mps\"\n",
    "        if torch.backends.mps.is_available()\n",
    "        else \"cpu\"\n",
    "    )\n",
    "\n",
    "\n",
    "get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### model ###\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    The Convolutional Neural Network implemented last tutorial session.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=3,\n",
    "                out_channels=8,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(\n",
    "                in_channels=8,\n",
    "                out_channels=16,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16 * 8 * 8, 10, bias=False),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### data loading ###\n",
    "\n",
    "train_data = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    ")\n",
    "test_data = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=8, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=8, shuffle=False)\n",
    "\n",
    "classes = (\n",
    "    \"plane\",\n",
    "    \"car\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2 Deconstructing the Basic Training Loop**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the beggining of training, we initialize the Model to random weights (which is done by just calling the constructor) and moving it to the optimal device (which is set in one of the recap-cells)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### initialize model, move to device ###\n",
    "\n",
    "model = Model()\n",
    "model = model.to(get_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then choose our loss function and optimizer. The loss function needs to be callable (in this case it is a callable object), and the optimizer is a `torch.optim.Optimizer` object, in this case Stochastic Gradient Descent. The optimizer must always be passed the parameters which it should optimize. We get those by calling `model.parameters()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### loss function and optimizer ###\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training loop, we first iterate over epochs, and secondly over a dataloader – the training loader for training the model, obviously. We unpack the batch provided by the dataloader to labels and images (inputs). `optimizer.zero_grad()` sets the previously calculated gradients to zero. We then send the inputs to the models and calculate the loss function using the labels and model outputs. We then backpropagate the loss by calling `loss.backward()`, and optimize the parameters by calling `optimizer.step()`.\n",
    "\n",
    "We record the batch `idx` using `enumerate` to be able to calculate average losses for recording training information.\n",
    "\n",
    "At the end of training, we save the model to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### training loop ###\n",
    "\n",
    "print(\"Starting Training\")\n",
    "model.train()\n",
    "\n",
    "for epoch in range(10):  # Limit to 10 epochs to keep the runtime short\n",
    "    sum_loss = 0.0\n",
    "\n",
    "    for idx, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data[0].to(get_device()), data[1].to(get_device())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        sum_loss += loss.item()\n",
    "        if idx % 1000 == 999:\n",
    "            print(f\"Epoch {epoch + 1}, batch {idx + 1}: loss {sum_loss / 1000:.3f}\")\n",
    "            sum_loss = 0.0\n",
    "\n",
    "print(\"Finished Training\")\n",
    "\n",
    "\n",
    "### save the model ###\n",
    "\n",
    "model.eval()\n",
    "torch.save(model.state_dict(), \"cifar10_cnn.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code loads the weights from the save-file, so that you can repeat evaluation without re-training the model when coming back to this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load the model ###\n",
    "\n",
    "model = Model()\n",
    "model.load_state_dict(torch.load(\"cifar10_cnn.pth\", weights_only=True))\n",
    "model = model.to(get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluate ###\n",
    "\n",
    "def eval(data: torch.utils.data.DataLoader, model: nn.Module) -> float:\n",
    "    \"\"\"\n",
    "    Calculates accuracy of `model` on the DataLoader `data`\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        true, pred = [], []\n",
    "        for batch in data:\n",
    "            images, labels = batch[0].to(device), batch[1].to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            true.extend(labels.to(\"cpu\"))\n",
    "            pred.extend(predicted.to(\"cpu\"))\n",
    "\n",
    "    return accuracy_score(true, pred)\n",
    "\n",
    "\n",
    "print(f\"Test Accuracy: {eval(test_loader, model):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3 Building a Better Trianing Loop**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.1 Optimizers: `Adam`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is Adam?** [Adam](https://paperswithcode.com/method/adam) is an optimization algorithm (Kingma & Ba, 2015). Since then, it has found wide application for optimizing neural network parameters. It extends Stochastic Gradient Descent with both Momentum (regulated by $\\beta_1$) and Root Mean Square Propagation (RMSprop), which essentially adapts the learning rate for each to-be-optimized parameter individually (regulated by $\\beta_2$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "model = Model()  # Create new untrained model, so that we can use this optimizer in training later (otherwise we would have to copy-paste this code)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    params=model.parameters(),  # parameters to optimize\n",
    "    lr=0.001,  # learning rate\n",
    "    betas=(\n",
    "        0.9,\n",
    "        0.999,\n",
    "    ),  # beta 1 momentum factor, beta 2 is RMSprop factor for per-parameter learning rate adjustment\n",
    "    eps=1e-8,  # parameter avoiding RMSprop denominator collapse\n",
    "    weight_decay=0,  # Factor to way in the L2 Norm (Euclidean distance) of all weights, i.e. not only minimize loss, but also weight values\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.2 Learning Rate Scheduling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training performance can be increased by adapting the learning rate, with techniques like learning rate warmup (not starting with the full learning rate but increasing it over the first episodes) or decay (reducing the learning rate as training goes on). A very effective technique demonstrated for example in the [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) paper (He et al., 2016). See how the classification error decreases visibly after reducing the learning rate:\n",
    "\n",
    "<image src='images/lr_schedule.png' style='width:800px'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### learning rate scheduling ###\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    optimizer=optimizer,  # the optimizer for which learning rate should be adapted\n",
    "    mode=\"max\",  # whether the metric should increase or decrease. Default is 'min' to be used with loss, when used with acc should be 'max'\n",
    "    patience=3,  # how long a metric must stop improving by at least best * (1 +/- threshold) per default, can also be set to absolute threshold mode\n",
    "    threshold=1e-4,  # Threshold for patience calculation\n",
    "    factor=0.1,  # The value to multiply the learning rate with\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate scheduling should be applied after the optimizer's update by calling `lr_scheduler.step()`. This should usually happen after iterating through the training DataLoader. Note that the `ReduceLROnPlateau` scheduler's `step()` method must be passed a value to track, i.e. which must plateau for learning rate to be reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.3 Validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To track training progress, looking only at the loss is not the best option. While we describe a performance criterion using the loss, it does not tell us as much about model _performance_ as say a calculated accuracy on a validation dataset would. Let's do exactly that!\n",
    "\n",
    "We already have a function for calculating accuracy given a model and a DataLoader. Now we just need to split our dataset, and torch has a function exactly for that: `torch.utils.data.random_split()`. Note that this task (splitting an existing dataset) is different from the one presented in the assignment, where we want to build the split into the Dataset class itself.\n",
    "\n",
    "**Note: Validation Dataset.** For datasets that are intended to be used as benchmarks, the test dataset is often either provided without labels or not provided at all. This is done to prevent models from being trained on the test data to cheat on the leaderboards. The validation dataset is thus all we have to get an estimate of how well our model performs.  \n",
    "In such cases, we should not use the validation dataset for the classic validation tasks (e.g., to calculate metrics for early stopping), but rather treat it as our test dataset. Instead, it is advisable to create our own validation dataset from the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create validation set from train set ###\n",
    "\n",
    "train, val = torch.utils.data.random_split(\n",
    "    dataset=train_data,  # Dataset object to split\n",
    "    lengths=(0.85, 0.15),  # Fractions of the returned datasets, must sum to 1\n",
    "    generator=torch.Generator().manual_seed(\n",
    "        2025\n",
    "    ),  # Ensures reproducibility, optional parameter\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=8, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.4 Checkpointing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `torch.save()` you can save components of your model and training process by passing a `dictionary`. Such checkpoint files are by convention ending in either `.pth` or `.pt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### checkpointing ###\n",
    "\n",
    "def save(save_dict: dict, path: str) -> None:\n",
    "    \"\"\"\n",
    "    Wrapper around torch.save, to demonstrate the syntax. This function may be used in the loop, or its content itself.\n",
    "    \"\"\"\n",
    "\n",
    "    torch.save(save_dict, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common dictionary to save could be the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"epoch\": epoch,\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    \"scheduler_state_dict\": lr_scheduler.state_dict(),\n",
    "    \"loss\": loss,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...which can then be loaded using torch.load, and be accessed like the originally saved dictionary. Models, optimizers and schedulers have `.load_state_dict()` methods to load the stored `state_dict`s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.5 Progress Bars!!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tqdm` Library offers progress bars for the console which display progress (duh) and can also show some information. We usually use progress bars around the DataLoaders rather than the epoch counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### default progress bar ###\n",
    "\n",
    "for i in tqdm(range(1000000)):\n",
    "    time.sleep(3e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### customizable progress bar ###\n",
    "\n",
    "pbar = tqdm(range(30000))\n",
    "\n",
    "for i in pbar:\n",
    "    pbar.set_description(f\"i: {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### syntactic sugar ###\n",
    "\n",
    "for i in (pbar := tqdm(range(30000), ncols=100)):  # ncols sets the width of the bar\n",
    "    pbar.set_description(f\"i = {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.6 Putting It All Together**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Build a training loop using the improvements shown in this notebook. Use validation accuracy for learning rate scheduling. Train for 10 epochs, and save a detailed checkpoint every five epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Load the checkpoint from epoch 10 for the model, optimizer and learning rate scheduler, and continue the training for 10 more epochs. You may use the same training loop as in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Evaluate the final model on the test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note in the case above the model produced after epoch 20 may not be the best model we have seen during training, judging by the validation accuracy. For all we know, a model from a previous epoch should perform better. For this purpose, we may also introduce a second set of checkpoints, which always store (and overwrite) a model whenever a new maximum validation accuracy is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **What Else?**\n",
    "\n",
    "Of course, this isn't everything to learn about PyTorch! Here is an (incomplete) list of resources for you to look at if you want to dive deeper into this framework! Note that some of these are really advanced – so don't worry if you don't understand them, you won't need them for this course.\n",
    "\n",
    "- [Tensorboard visualization of training metrics](https://docs.pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html)\n",
    "- [Distributed Data Parallel (DDP), i.e. Multi-GPU Training](https://docs.pytorch.org/tutorials/intermediate/ddp_tutorial.html)\n",
    "- Technically not PyTorch, but an important tool for academic experiments: Running experiments from configs, e.g. via [YACS](https://github.com/rbgirshick/yacs) or [JSON argparse](https://jsonargparse.readthedocs.io/en/v4.44.0/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Bibliography**\n",
    "\n",
    "Kingma, D. P., & Ba, J. (2015). Adam: A Method for Stochastic Optimization. In Y. Bengio & Y. LeCun (Hrsg.), 3rd international conference on learning representations, ICLR 2015, san diego, CA, USA, may 7-9, 2015, conference track proceedings. http://arxiv.org/abs/1412.6980\n",
    "\n",
    "He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep Residual Learning for Image Recognition. 770–778. https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2526",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
