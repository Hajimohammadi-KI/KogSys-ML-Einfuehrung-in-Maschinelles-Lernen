{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a835d39",
   "metadata": {},
   "source": [
    "<img src=\"images/logo.png\" style=\"width: 100px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94977d6",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "### Decision Trees, K-Fold Cross-Validation and ILP\n",
    "\n",
    "_Submission deadline: **23.11.2025**_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2e7abc",
   "metadata": {},
   "source": [
    "#### Submission Information\n",
    "\n",
    "Upload your solution via the VC course. Please upload **one zip archive per group**. It must contain:\n",
    "\n",
    "- Your solution as a **notebook** (a `.ipynb` file) and/or Python files (`.py`).\n",
    "- A folder **images** with all your images (keep image sizes reasonably small)\n",
    "\n",
    "Your zip file should be named according to the following scheme:\n",
    "\n",
    "```\n",
    "assignment_<assignment number>_solution_<group number>.zip\n",
    "```\n",
    "\n",
    "In this assignment you can achieve a total of **62** points. These points translate into **2.5 bonus points** for the exam as follows:\n",
    "\n",
    "| **Assignment points** | **Exam bonus points** |\n",
    "| :-: | :-: |\n",
    "| 59 | 2.5 |\n",
    "| 50 | 2.0 |\n",
    "| 41 | 1.5 |\n",
    "| 32 | 1.0 |\n",
    "| 23 | 0.5 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc63dabb",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-danger'>\n",
    "\n",
    "##### **Important Notes**\n",
    "\n",
    "1. **This assignment is graded. You can earn bonus points for the exam.**\n",
    "2. **If it is obvious that a solution was copied from another source without your own contribution, we will not award bonus points. Write all answers in your own words!**\n",
    "3. **If LLMs (e.g., ChatGPT or Copilot) were used to create your submission, please indicate this at the respective places. Also note the [AI policy](https://cogsys.uni-bamberg.de/teaching/ki-richtlinie.html).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278f3b09",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779301d3",
   "metadata": {},
   "source": [
    "For task 3, you must also have [SWI Prolog](https://www.swi-prolog.org) installed on your system; otherwise installing the dependencies will fail!\n",
    "\n",
    "You can also install _SWI Prolog_ via common package managers.\n",
    "\n",
    "- **Windows (chocolatey):** `choco install swi-prolog`\n",
    "- **macOS (Homebrew):** `brew install swi-prolog`\n",
    "\n",
    "**Important:** If you install _SWI Prolog_ manually, don't forget to add it to your `PATH` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b53867a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages with the currently selected Python interpreter\n",
    "%pip install -U -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dab9b8f",
   "metadata": {},
   "source": [
    "### 1 | ID3 Implementation\n",
    "\n",
    "_Worth a total of 26 points_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a70ce86",
   "metadata": {},
   "source": [
    "In autumn, forests turn golden and mushroom season begins in many places. Among the colorful leaves you can find many mushroom species. Some are edible, others highly poisonous. An automated method to determine edibility can help avoid dangerous mix-ups. Decision trees can be used for this.\n",
    "\n",
    "In this task, you will implement a decision tree using the **ID3 algorithm** to decide whether a mushroom is _edible_ or _poisonous_ based on mushroom features. The goal is to implement the **ID3 algorithm** for classification using the `mushrooms.csv` dataset. The algorithm should learn a decision tree from the given features to predict the target variable `dangerous` as accurately as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32f8e6b",
   "metadata": {},
   "source": [
    "**_Importing libraries._** The following cell imports some important libraries needed for the implementation:\n",
    "- `random` and `numpy`: We use these to set _seeds_ so that results of random generators are reproducible.\n",
    "- `pandas`: We use DataFrames to hold the datasets.\n",
    "- `typing.Any`: For type hints in method signatures.\n",
    "- `sklearn.base.ClassifierMixin`: Provides common functionality for classification models.\n",
    "- `sklearn.model_selection.train_test_split`: Splits datasets into training and test sets.\n",
    "- `sklearn.metrics.accuracy_score`: Computes the fraction of correctly classified examples.\n",
    "\n",
    "**_Nothing needs to be changed in the next code cell._**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb99d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Any\n",
    "from numpy.typing import ArrayLike\n",
    "\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "random.seed(2025)\n",
    "np.random.seed(2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8b1c93",
   "metadata": {},
   "source": [
    "**_Importing datasets._** The following cell loads the dataset used in this assignment. It is stored in `mushrooms.csv` and contains six categorical decision attributes and one binary target attribute:\n",
    "\n",
    "- `habitat` (`forest`, `field`, `underwater`)\n",
    "- `hat_color` (`red`, `white`, `brown`)\n",
    "- `size` (`small`, `medium`, `large`)\n",
    "- `lamellae` (`wide`, `narrow`, `none`)\n",
    "- `hat_form` (`steep`, `flat`)\n",
    "- `stem` (`rough`, `smooth`)\n",
    "- **Target:** `dangerous` (`True`, `False`)\n",
    "\n",
    "After reading the `.csv` into a `pd.DataFrame`, it is split into `X` (only decision attributes) and `y` (only target attribute). `X` and `y` are then split into a training set of $80\\%$ and a test set of $20\\%$ using `train_test_split()`. **Important:** with `random_state=2025` we ensure that the splits are always the same on each run. Scikit-learn uses its own seed separate from `numpy` or `random`.\n",
    "\n",
    "**_Nothing needs to be changed in the next code cell._**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfff7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mushrooms = pd.read_csv('mushrooms.csv')\n",
    "\n",
    "X = mushrooms.drop('dangerous', axis=1)\n",
    "y = mushrooms['dangerous']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d02261",
   "metadata": {},
   "source": [
    "#### **(01.1.1)** Computing Entropy and Information Gain\n",
    "\n",
    "To build the decision tree, two fundamental functions are needed:\n",
    "\n",
    "1. `entropy(y)`: Computes the degree of uncertainty in the target distribution\n",
    "2. `information_gain(X, y, attribute)`: Measures how much a particular attribute reduces uncertainty and is thus relevant for the split.\n",
    "\n",
    "_You can implement these functions in the following code block. You may also use a separate Python file if you prefer._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee54a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y: ArrayLike) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the entropy for the example subset `y`. Note that this method uses `y`, because entropy is always calculated with respect to the target attribute, so this method only needs the target attribute.\n",
    "\n",
    "    #### Parameters\n",
    "    - `y (ArrayLike)`: Subset of the target attribute for which to calculate entropy.\n",
    "\n",
    "    #### Returns\n",
    "    - `_ (float)`: Entropy calculated for `y`.\n",
    "    \"\"\"\n",
    "    # TODO: Compute the entropy of the example subset\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471025dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(X: ArrayLike, y: ArrayLike, attribute: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the information gain for the attribute `attribute` given the target attribute `y`.\n",
    "\n",
    "    #### Parameters\n",
    "    - `X (ArrayLike`: The full dataset as array like structure (at this level of recursion).\n",
    "    - `y (ArrayLike)`: The corresponding target attribute.\n",
    "    - `attribute (str)`: The attribute for which to calculate the information gain.\n",
    "\n",
    "    #### Returns\n",
    "    - `_ (float)`: Information gain calculated for `attribute`.\n",
    "    \"\"\"\n",
    "    # TODO: Compute the information gain\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea08b44a",
   "metadata": {},
   "source": [
    "#### **(01.1.2)** Implementing the ID3 Algorithm\n",
    "\n",
    "You will now implement a class `DecisionTree` that implements the **ID3 algorithm**. The algorithm should recursively select the attribute with the highest information gain and thus iteratively build a decision tree. The tree structure can be chosen freely (e.g., as a nested dictionary or via your own classes). The trained tree should then be used with the previously discussed `mushrooms.csv` dataset to make predictions about the _edibility_ of mushrooms.\n",
    "\n",
    "The class `DecisionTree` should inherit from `ClassifierMixin`. The methods `fit` and `predict` are required. This allows your decision tree to be used in pipelines based on the `sklearn` framework.\n",
    "\n",
    "_It is not necessary to implement this in the Jupyter notebook. You can also use separate Python files._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d098822",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ID3 Implementation ###\n",
    "\n",
    "class DecisionTree(ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Implements a tree data structure and both decision tree training using ID3 and decision tree inference. Also includes methods for calculating entropy and information gain.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, default_class: Any = None):\n",
    "        \"\"\"\n",
    "        Decision Tree constructor\n",
    "        \"\"\"\n",
    "        \n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    def fit(self, X: ArrayLike, y: ArrayLike):\n",
    "        \"\"\"\n",
    "        Fit the decision tree to the dataset `X` with target attribute `y`.\n",
    "\n",
    "        #### Parameters\n",
    "        - `X (v)`: The full dataset.\n",
    "        - `y (ArrayLike)`: The corresponding target attribute.\n",
    "        \"\"\"\n",
    "        \n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    def id3(self, X: ArrayLike, y: ArrayLike) -> dict[dict] | Any:\n",
    "        \"\"\"\n",
    "        Recursively build the decision tree using the ID3 algorithm.\n",
    "\n",
    "        #### Parameters\n",
    "        - `X (ArrayLike)`: The full dataset (at this level of recursion).\n",
    "        - `y (ArrayLike)`: The corresponding target attribute.\n",
    "\n",
    "        #### Returns\n",
    "        - `_ (dict[dict] | Any)`: The resulting Decision Tree as a nested dictionary. Each node is either a nested dictionary (internal node) or a leaf node with the predicted value.\n",
    "        \"\"\"\n",
    "        \n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    def predict(self, X: ArrayLike) -> ArrayLike:\n",
    "        \"\"\"\n",
    "        Predict the target attribute for the dataset `X`.\n",
    "\n",
    "        #### Parameters\n",
    "        - `X (ArrayLike)`: The dataset for which to predict the target attribute.\n",
    "\n",
    "        #### Returns\n",
    "        - `_ (ArrayLike)`: The predicted target attributes.\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7209fceb",
   "metadata": {},
   "source": [
    "#### **(01.1.3)** Training\n",
    "\n",
    "After implementing the decision tree, apply it to the `mushrooms.csv` dataset. In the following code cells you should:\n",
    "\n",
    "1. **Train the decision tree** with the prepared training data\n",
    "2. **Predict** the class membership on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d396c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Decision Tree\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cacccff",
   "metadata": {},
   "source": [
    "### 2 | ID3 Evaluation with k-Fold Cross-Validation\n",
    "\n",
    "_Worth a total of 20 points_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1c4884",
   "metadata": {},
   "source": [
    "After implementing the **ID3 algorithm**, you should now evaluate the developed decision tree (`DecisionTree`). The goal is to determine the model quality of the tree and to check how stable the results are across different data splits.\n",
    "\n",
    "#### **(01.2.1)** k-Fold Cross-Validation\n",
    "\n",
    "Use **k-Fold Cross-Validation** for this. This method splits the dataset into multiple subsets (folds) and allows the decision tree to be evaluated multiple times with different training and test splits. Repeated training and testing yields a more comprehensive picture of model performance than a single split.\n",
    "\n",
    "**Using `KFold` or similar helper classes from scikit-learn is not allowed. The splitting into folds must be implemented manually.**\n",
    "\n",
    "For this task, implement two functions that automate the evaluation process together:\n",
    "\n",
    "1. `evaluate_fold`. This function evaluates a single fold. It trains the decision tree (`DecisionTree`) on the training data and then checks its accuracy on the test data. If your own implementation of `DecisionTree` is not functional, you may fall back to `DecisionTreeClassifier` from `scikit-learn` to complete the evaluation process. In that case, categorical inputs must be encoded beforehand.\n",
    "2. `run_kfold_evaluation`. This function orchestrates the entire cross-validation. It creates the $k$ folds ($k=5$) of the dataset. The data should first be randomly shuffled to ensure a fair class distribution. Then, build the index groups for the five folds, where one fold serves as test set and the remaining as training set. For each fold, call the previously defined `evaluate_fold` function. Collect all individual results in a table. The resulting table should include fold number, training and test sizes, method used, and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1ec02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fold(\n",
    "        X_train: ArrayLike,\n",
    "        X_test: ArrayLike,\n",
    "        y_train: ArrayLike,\n",
    "        y_test: ArrayLike,\n",
    "        fold: int\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Evaluates a single fold during a cross-validation process. The function trains the decision tree on the given training data, makes\n",
    "    predictions on the test data, and calculates the accuracy of these predictions.\n",
    "\n",
    "    #### Parameters\n",
    "    - `X_train (ArrayLike)`: Training feature dataset.\n",
    "    - `X_test (ArrayLike)`: Test feature dataset.\n",
    "    - `y_train (ArrayLike)`: Training target labels.\n",
    "    - `y_test (ArrayLike)`: Test target labels.\n",
    "    - `fold (int)`: Fold identifier.\n",
    "\n",
    "    #### Returns\n",
    "    - `dict`: A dictionary containing:\n",
    "        - `'fold' (int)`: The fold identifier.\n",
    "        - `'train_size' (int)`: Number of samples in the training set.\n",
    "        - `'test_size' (int)`: Number of samples in the test set.\n",
    "        - `'accuracy' (float)`: Accuracy score for the current fold.\n",
    "    \"\"\"\n",
    "    # TODO: Implement the evaluation of a single fold.\n",
    "    pass\n",
    "\n",
    "\n",
    "def run_kfold_evaluation(X: ArrayLike, y: ArrayLike, k=5, random_state=2025) -> ArrayLike:\n",
    "    \"\"\"\n",
    "    Performs k-fold cross-validation on the given dataset using a custom decision tree evaluator.\n",
    "\n",
    "    The function splits the dataset into `k` folds, trains and tests a decision tree model on each fold using the `evaluate_fold` function, and aggregates the results (e.g., accuracy) into a single DataFrame.\n",
    "\n",
    "    #### Parameters\n",
    "    - `X (ArrayLike)`: Feature dataset containing independent variables.\n",
    "    - `y (ArrayLike)`: Target variable corresponding to `X`.\n",
    "    - `k (int, default=5)`: Number of folds to use for cross-validation.\n",
    "    - `random_state (int, default=2025)`: Random seed for reproducibility of fold splits.\n",
    "\n",
    "    #### Returns\n",
    "    - `ArrayLike`: A DataFrame summarizing the evaluation results for each fold,\n",
    "      containing the following columns:\n",
    "        - `'fold' (int)`: The fold identifier.\n",
    "        - `'train_size' (int)`: Number of training samples in that fold.\n",
    "        - `'test_size' (int)`: Number of test samples in that fold.\n",
    "        - `'accuracy' (float)`: Accuracy score achieved on the test set.\n",
    "    \"\"\"\n",
    "    # TODO: Implement the k-fold cross-validation process.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a91d83",
   "metadata": {},
   "source": [
    "#### **(01.2.2)** Interpretation of the Results\n",
    "\n",
    "The following cell runs the previously implemented k-fold evaluation and prints a tabular overview of the results.\n",
    "\n",
    "**_Nothing needs to be changed in the next code cell._**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80144ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = run_kfold_evaluation(X, y, k=5)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1f6fce",
   "metadata": {},
   "source": [
    "What stands out in the results? Briefly describe any patterns or peculiarities you observe. In particular, discuss what the results say about the model's ability to generalize. Relate this to the training from subtask 01.1.3. What do the results say about your implementation of the `DecisionTree` class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded9c148",
   "metadata": {},
   "source": [
    "> _Interpretation of the results:_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27eea512",
   "metadata": {},
   "source": [
    "Is accuracy a suitable metric here? What alternatives are there?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1e04b9",
   "metadata": {},
   "source": [
    "> _Discussion of Metrics:_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4787746",
   "metadata": {},
   "source": [
    "### 3 | ILP\n",
    "_Worth a total of 16 points_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155d1eae",
   "metadata": {},
   "source": [
    "A witch spent her entire life documenting rituals. In her digitized ritual journal, more than seventy rituals are recorded—some summoned dangerous monsters, while others were harmless. Each ritual is described precisely: the ingredients used, the location, the moon phase, and special aspects such as whether the incantations were spoken backwards or the ritual took place at midnight.\n",
    "\n",
    "Your task is to develop a Python program that, using `janus_swi`, queries the Prolog facts in `rituals.pl` and analyzes which conditions are most strongly associated with dangerous summons.\n",
    "Here, the _FoilGain_ known from the lecture should be computed.\n",
    "\n",
    "The FoilGain formula is:\n",
    "\n",
    "$$\n",
    "\\text{FoilGain}(L, R) = t \\cdot \\Big(\\log_2\\frac{p_1}{p_1+n_1} - \\log_2\\frac{p_0}{p_0+n_0}\\Big)\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "- $L$ as the new literal added to ($R$) to obtain the new rule ($R'$),\n",
    "- $t$ the number of positive examples of the rule ($R$) that are still covered by ($R'$),\n",
    "- $p_0$ the number of positive examples covered **before** adding ($L$),\n",
    "- $n_0$ the number of negative examples covered **before** adding ($L$),\n",
    "- $p_1$ the number of positive examples covered **after** adding ($L$),\n",
    "- $n_1$ the number of negative examples covered **after** adding ($L$).\n",
    "\n",
    "The FoilGain formula evaluates _how much a new condition (literal)_ improves the separation between dangerous and harmless rituals.\n",
    "A high FoilGain value means that the literal significantly increases the proportion of dangerous rituals among the covered examples—and is therefore an important indicator of dangerous summoning conditions.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Procedure and requirements for the implementation:**\n",
    "\n",
    "* Use `janus_swi` to query the facts in `rituals.pl`.\n",
    "\n",
    "* Implement a function `calculate_foil_gain(p0, n0, p1, n1)` that computes FoilGain according to the formula given.\n",
    "\n",
    "* Create a helper function that, for an arbitrary condition (literal), counts how many rituals are dangerous and how many are safe.\n",
    "\n",
    "* The possible conditions (candidate literals) must _not be hardcoded_ in the code. Instead, they should be generated _automatically_ from the existing facts in `rituals.pl`.\n",
    "  This keeps the program extensible if new `ingredients`, `moon_phases` or `locations` are added.\n",
    "\n",
    "- The Prolog queries can, for example, look like this:\n",
    "  `\"summoned(R, monster).\"` for dangerous rituals or `\"ingredient(R, nightshade).\"` as an additional condition.\n",
    "  Thus, rituals with `\"summoned(R, monster).\"` count as _positive examples_, and those with `\"summoned(R, nothing).\"` as _negative examples_.\n",
    "\n",
    "- Then implement a routine that _automatically generates all possible candidate literals_ and, for each literal:\n",
    "  1. determines ($p_0$, $n_0$, $p_1$, $n_1$),\n",
    "  2. computes the FoilGain,\n",
    "  3. stores the result (literal + FoilGain value) in a list or table.\n",
    "\n",
    "- Sort all computed conditions by their FoilGain value.\n",
    "\n",
    "- Finally, print the _five most dangerous conditions_ (with the highest FoilGain).\n",
    "\n",
    "- Test your implementation with selected conditions, e.g., certain ingredients, moon phases, locations, or special attributes (`spoken_backwards`, `performed_at_midnight`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06aaf091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e138c0",
   "metadata": {},
   "source": [
    ">"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
