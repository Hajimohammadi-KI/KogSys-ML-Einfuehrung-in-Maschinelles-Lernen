{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a429f96",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "### MLP and $k$-distance\n",
    "\n",
    "_Submission deadline: **14.12.2025**_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0091e3eb",
   "metadata": {},
   "source": [
    "#### Submission Information\n",
    "\n",
    "Upload your solution via the VC course. Please upload **one zip archive** per group. This must contain:\n",
    "\n",
    "- Your solution as a **Notebook** (a `.ipynb` file)\n",
    "- At least the file ``MLP.py`` for task 02.1.1.\n",
    "- A folder **images** with all your images (keep the image sizes relatively small)\n",
    "\n",
    "Your zip file should be named according to the following scheme:\n",
    "\n",
    "```\n",
    "assignment_<assignment number>_solution_<group number>.zip\n",
    "```\n",
    "\n",
    "In this assignment you can achieve a total of **55** points. These points translate into **2.5 bonus points** for the exam as follows:\n",
    "\n",
    "| **Points in Assignment** | **Bonus Points for Exam** |\n",
    "| :-: | :-: |\n",
    "| 52 | 2.5 |\n",
    "| 44 | 2.0 |\n",
    "| 36 | 1.5 |\n",
    "| 28 | 1.0 |\n",
    "| 20 | 0.5 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cae74c",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-danger'>\n",
    "\n",
    "##### **Important Notes**\n",
    "\n",
    "1. **This assignment will be graded. You can earn bonus points for the exam.**\n",
    "2. **If it is obvious to us that a task was copied from another source and no independent work was performed, we will not award any bonus points. Formulate all answers in your own words!**\n",
    "3. **If LLMs (such as ChatGPT or CoPilot) were used to create your submission, please indicate this at the respective places. Also observe the [AI-Policy](https://cogsys.uni-bamberg.de/teaching/ki-richtlinie.html).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59e0441",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf8ca24",
   "metadata": {},
   "source": [
    "### 1 | MLP\n",
    "\n",
    "_For a total of 37 points_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02f26d6",
   "metadata": {},
   "source": [
    "### **(02.1.1)** Backpropagation\n",
    "\n",
    "_For **31** points_\n",
    "\n",
    "In this task, you should develop your own Python package that implements a Multilayer Perceptron (MLP). You will later use this package in the notebook to classify the Iris dataset. The goal of the task is to understand and implement the basic computation steps of a neural network with at least one hidden layer yourself. A simple Perceptron is already available in the file `perceptron.py`. First, familiarize yourself with the implementation of this file.\n",
    "\n",
    "The focus in the following is on implementing the backpropagation algorithm. For this, you should program two methods in the file `MLP.py`:\n",
    "\n",
    "- `fit`: This method should perform the actual training process over multiple epochs.\n",
    "- `backprop`: This method should calculate gradients for all weights and biases.\n",
    "\n",
    "The signatures as well as additional methods are already contained in `MLP.py`. Familiarize yourself with these first. You will need them for your following implementation. A special feature is the one-hot encoding of class labels, which is done by the `__one_hot` method. One-hot encoding is a simple method to represent categories numerically. This is necessary to output an activation in the form of a vector at the end.\n",
    "\n",
    "To enable a fast but still algorithm-oriented implementation, the weights are stored twice: once in the individual perceptrons of the layers, and once as a collected weight matrix that enables fast calculations. In order for your weight adjustments to be implemented, update the weights of the individual perceptrons (``Perceptron.update_weights()``) according to the backpropagation algorithm, and use the method ``MLP.update_matrices()`` to transfer the new weights into the matrices before you calculate a forward pass again.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba870e56",
   "metadata": {},
   "source": [
    "### **(02.1.2)** Test Implementation with Iris Dataset\n",
    "\n",
    "_For **6** points_\n",
    "\n",
    "Once you have implemented the previous implementation, the `KogSysMLP` Python package can be installed using `pip`. To do this, change to the directory `KogSysMLP/` and execute the following command:\n",
    "\n",
    "```bash\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "In the next code cell, the Python package just installed is loaded. The following libraries are also imported:\n",
    "\n",
    "- `sklearn.datasets.load_iris`: Loads the Iris dataset.\n",
    "- `sklearn.model_selection.train_test_split`: Splits datasets into training and test data.\n",
    "- `sklearn.preprocessing.StandardScaler`: Scales the features of the dataset.\n",
    "\n",
    "**_Nothing needs to be changed in the next code cell._**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d944a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "\n",
    "import KogSysMLP\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e6a2ab",
   "metadata": {},
   "source": [
    "Now we will test with the Iris dataset. For this purpose, it is loaded in the following cell.\n",
    "\n",
    "**_Nothing needs to be changed in the next code cell._**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19823ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931bfd77",
   "metadata": {},
   "source": [
    "First scale the dataset with the `StandardScaler` and then split it using `train_test_split` into training and test data. Use 20% of the data as the test set and a `random_state = 42` so that the split remains reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0cbe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facf0a97",
   "metadata": {},
   "source": [
    "Now initialize the implemented MLP. First test the parameters yourself. Try to maximize the accuracy, which you should implement in the following code block. A learning rate in the low range (e.g., between 0.01 and 0.1) and several thousand epochs are often a sensible starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6121729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96b30c4",
   "metadata": {},
   "source": [
    "After training, calculate the accuracy of your MLP on the test data. Use the model's predictions and compare them with the actual labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbf2469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71400bf3",
   "metadata": {},
   "source": [
    "---\n",
    "### 2 | Local Outlier Factor\n",
    "\n",
    "_For a total of **18** points_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c55a6f5",
   "metadata": {},
   "source": [
    "To detect outliers from clusters, a $k$-distance approach can be implemented based on the $k$-Nearest-Neighbors algorithm. The basic principle of this so-called Local Outlier Factor can be read [here in the Wikipedia article](https://en.wikipedia.org/wiki/Local_Outlier_Factor). Familiarize yourself with this approach!\n",
    "\n",
    "The goal of this task is to implement the $k$-distance algorithm.\n",
    "\n",
    "At the end of the task there is a test block with which you can test your implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0b6692",
   "metadata": {},
   "source": [
    "### **(02.2.1)** Determining the Distance\n",
    "\n",
    "_For **6** points_\n",
    "\n",
    "The distances between the individual points should be calculated according to their Euclidean distance. The Euclidean distance is defined as\n",
    "$$d(x_i, x_j) \\equiv \\sqrt{\\sum_{r=1}^{n}\\left(a_r(x_i) - a_r(x_j)\\right)^2}.$$\n",
    "\n",
    "To calculate this value, implement the function `euclidean_distance()`. The function `pairwise_distances()` should then calculate the pairwise Euclidean distances of a point set and return them as a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d860d1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def euclidean_distance(x1, x2):\n",
    "    return NotImplementedError\n",
    "\n",
    "def pairwise_distances(X):\n",
    "    return NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde3757d",
   "metadata": {},
   "source": [
    "### **(02.2.2)** k-Distance Implementation\n",
    "\n",
    "_For **12** points_\n",
    "\n",
    "Now implement the `k_distance` class. Base yourself on the fundamentals of the k-nearest-neighbor algorithm (as learned in the lecture and exercise) and adapt it with the formulas explained in the Wikipedia article ([Local Outlier Factor](https://en.wikipedia.org/wiki/Euclidean_distance)). \n",
    "\n",
    "The class should contain the methods `fit()` and `predict()`. Additional helper functions (for example, to calculate the values of the Local Outlier Factor) can make the class more manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208a7513",
   "metadata": {},
   "outputs": [],
   "source": [
    "class k_distance:\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.x_train = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"Stores training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training samples to be used for LOF computation.\n",
    "        \"\"\"\n",
    "        return NotImplementedError\n",
    "    \n",
    "    def predict(self):\n",
    "        \"\"\"Compute LOF scores for the stored training data.\n",
    "        Expects that fit() has been called before.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        lof : ndarray, shape (n_samples,)\n",
    "            Local Outlier Factor score for each sample.\n",
    "        neigh_idx : ndarray, shape (n_samples, k)\n",
    "            Indices of the k nearest neighbors for each sample.\n",
    "        \"\"\"\n",
    "        return NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5377a86",
   "metadata": {},
   "source": [
    "#### Test on Example Data\n",
    "\n",
    "Now your implemented algorithm should be applied! For this purpose, a training dataset `X` with two clusters and outliers is created in the following code cell, which should be detected by your k_distance class. Then the model is fitted to the training data and the LOF scores are created. If your algorithm was implemented correctly, you can see the indices of the outliers in the output, as well as in the subsequent plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9ebc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "cluster1 = rng.normal(loc=[0, 0], scale=0.5, size=(100, 2))\n",
    "cluster2 = rng.normal(loc=[5, 5], scale=0.5, size=(100, 2))\n",
    "outliers = np.array([[8, 0], [0, 8], [10, 10], [-5, -5], [6, -6]])\n",
    "\n",
    "X = np.vstack([cluster1, cluster2, outliers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edd84fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "model = k_distance(k=10)\n",
    "# fit model\n",
    "model.fit(X)\n",
    "# predict LOF scores\n",
    "lof_scores_arr, neighbors = model.predict()\n",
    "\n",
    "# print top 10 anomalies\n",
    "order = np.argsort(lof_scores_arr)[::-1]\n",
    "print(\"Top anomalies (index, LOF score):\")\n",
    "for idx in order[:10]:\n",
    "  print(idx, float(lof_scores_arr[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d45ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot data and highlight anomalies\n",
    "plt.scatter(X[:,0], X[:,1], s=12, label='data')\n",
    "plt.scatter(X[order[:10],0], X[order[:10],1], c='red', s=40, label='anomalies')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
