{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/logo.png\" style=\"width: 100px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "### MLP und $k$-distance\n",
    "\n",
    "_Abgabefrist: **14.12.2025**_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Informationen zur Abgabe\n",
    "\n",
    "Laden Sie Ihre Lösung über den VC-Kurs hoch. Bitte laden Sie pro Gruppe **ein Zip-Archiv** hoch. Dieses muss enthalten:\n",
    "\n",
    "- Ihre Lösung als **Notebook** (eine `.inpynb` Datei)\n",
    "- Mindestens die Datei ``MLP.py`` für Aufgabe 02.1.1.\n",
    "- Einen Ordner **images** mit allen Ihren Bildern (halten Sie die Größe der Bilder relativ klein)\n",
    "\n",
    "Ihre Zip-Datei sollte nach folgendem Schema benannt sein:\n",
    "\n",
    "```\n",
    "assignment_<assignment number>_solution_<group number>.zip\n",
    "```\n",
    "\n",
    "In diesem Assignment können Sie insgesamt **55** Punkte erreichen. Aus diesen Punkten berechnen sich **2,5 Bonuspunkte** für die Klausur wiefolgt:\n",
    "\n",
    "| **Punkte im Assignment** | **Bonuspunkte für die Klausur** |\n",
    "| :-: | :-: |\n",
    "| 52 | 2.5 |\n",
    "| 44 | 2.0 |\n",
    "| 36 | 1.5 |\n",
    "| 28 | 1.0 |\n",
    "| 20 | 0.5 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-danger'>\n",
    "\n",
    "##### **Wichtige Hinweise**\n",
    "\n",
    "1. **Dieses Assignment wird benotet. Sie können Bonuspunkte für die Prüfung erhalten.**\n",
    "2. **Wenn es für uns offensichtlich ist, dass eine Aufgabe von einer anderen Quelle kopiert wurde und keine Eigenleistung erbracht wurde, vergeben wir keine Bonuspunkte. Formulieren Sie alle Antworten in eigenen Worten!**\n",
    "3. **Wenn LLMs (wie z.B. ChatGPT oder CoPilot) verwendet wurden, um Ihre Abgabe zu erstellen, geben Sie dies bitte an den jeweiligen Stellen an. Beachten Sie zudem die [AI-Policy](https://cogsys.uni-bamberg.de/teaching/ki-richtlinie.html).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 | MLP\n",
    "\n",
    "_Für insgesamt 37 Punkte_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(02.1.1)** Backpropagation\n",
    "\n",
    "_Für **31** Punkte_\n",
    "\n",
    "In dieser Aufgabe sollst du ein eigenes Python-Package entwickeln, das ein Multilayer Perceptron (MLP) implementiert. Dieses Package wirst du später im Notebook verwenden, um den Iris-Datensatz zu klassifizieren. Ziel der Aufgabe ist es, die grundlegenden Berechnungsschritte eines neuronalen Netzes mit mindestens einer versteckten Schicht selbst zu verstehen und umzusetzen. Ein einfaches Perceptron ist bereits in der Datei `perceptron.py` vorhanden. Mache dich zuerst mit der Implementation dieser Datei vertraut.\n",
    "\n",
    "Der Schwerpunkt im folgenden liegt auf der Implementierung des Backpropagation-Algorithmus. Dafür sollst du zwei Methoden in der Datei `MLP.py` programmieren:\n",
    "\n",
    "- `fit`: Diese Methode soll den eigentlichen Trainingsprozess über merhere Epochen hinweg durchführen.\n",
    "- `backprop`: Diese Methode soll Gradienten für alle Gewichte und Biases berechnen.\n",
    "\n",
    "Die Signaturen sowie weitere Methoden sind bereits in `MLP.py` enthalten. Mache dich mit diesen zuerst vertraut. Du wirst sie für deine folgende Implementierung benötigen. Eine Besonderheit ist die One-Hot-Codierung der Klassenbezeichnungen, welche durch die `__one_hot` Methode erfolgt. Eine One-Hot-Codierung ist eine einfache Methode, um Kategorien numerisch darzustellen. Die ist notwendig, um am Ende eine Aktivierung in Form eines Vektors auszugeben.\n",
    "\n",
    "Um eine schnelle aber trotzdem Algorithmus-nahe Implementation zu ermöglichen, werden die Gewichte doppelt abgespeichert: Einmal in den einzelnen Perceptronen der Layer, und einmal als gesammelte Gewichtsmatrix, die schnelle Berechnungen ermöglicht. Damit eure Gewichtsanpassungen auch umgesetzt werden, aktualisiert die Gewichte der einzelnen Perceptronen (``Perceptron.update_weights()``) gemäß dem Backpropagation Algorithmus, und nutzt die Methode ``MLP.update_matrices()`` um die neuen Gewichte in die Matrizen zu übertragen bevor ihr erneut einen Forward-Pass berechnet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(02.1.2)** Implementierung mit Iris-Datensatz testen\n",
    "\n",
    "_Für **6** Punkte_\n",
    "\n",
    "Sofern du die vorherige Implementierung nun umgesetzt hast, kann das `KogSysMLP` Python Package mittels `pip` installiert werden. Wechsel dazu in das Verzeichnis `KogSysMLP/` und führe folgenden Befehl aus:\n",
    "\n",
    "```bash\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "In der nächsten Code Zelle wird das soeben installierte Python Package geladen. Ebenso werden die folgenden Bibliotheken importiert:\n",
    "\n",
    "- `sklearn.datasets.load_iris`: Lädt den Iris-Datensatz.\n",
    "- `sklearn.model_selection.train_test_split`: Teilt Datensätze in Trainings- und Testdaten auf.\n",
    "- `sklearn.preprocessing.StandardScaler`: Skaliert die Merkmale des Datensatzes.\n",
    "\n",
    "**_An der nächsten Code Zelle muss nichts verändert werden._**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "\n",
    "import KogSysMLP\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun soll mit dem Iris-Datensatz getestet werden. Dazu wird dieser in der folgendne Zelle geladen.\n",
    "\n",
    "**_An der nächsten Code Zelle muss nichts verändert werden._**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skaliere den Datensatz zunächst mit dem `StandardScaler` und teile ihn anschließend mithilfe von `train_test_split` in Trainings- und Testdaten auf. Verwende dabei 20% der Daten als Testmenge und einen `random_state = 42`, sodass die Aufteilung reproduzierbar bleibt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialisiere nun das implementierte MLP. Teste die Parameter zunächst selbst. Versuche dabei die Accuracy, welche du im nachfolgenden Code Block implementieren sollst, zu maximieren. Eine Lernrate im niedrigen Bereich (z. B. zwischen 0.01 und 0.1) und einige tausend Epochen sind oft ein sinnvoller Startpunkt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berechne nach dem Training die Genauigkeit deines MLP auf den Testdaten. Nutze dazu die Vorhersagen des Modells und vergleiche sie mit den tatsächlichen Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2 | Local Outlier Factor\n",
    "\n",
    "_Für insgesamt **18** Punkte_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit Ausreißer von Clustern erkannt werden, kann basierend auf dem $k$-Nearest-Neighbors Algorithmus ein $k$-Distance-Ansatz implementiert werden. Das Grundprinzip dieses sog. Local Outlier Factors ist [hier im Wikipedia-Artikel](https://en.wikipedia.org/wiki/Local_Outlier_Factor) nachzulesen. Macht euch mit diesem Ansatz vertraut!\n",
    "\n",
    "Ziel dieser Aufgabe ist es, den $k$-Distance Algorithmus zu implementieren.\n",
    "\n",
    "Am Ende der Aufgabe befindet sich ein Testblock, mit dem ihr eure Implementierung testen könnt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(02.2.1)** Ermittlung der Distanz\n",
    "\n",
    "_Für **6** Punkte_\n",
    "\n",
    "Die Abstände zwischen den einzelnen Punkten sollen gemäß ihrer euklidischen Distanz berechnet werden. Die euklidische Distanz ist definiert als\n",
    "$$d(x_i, x_j) \\equiv \\sqrt{\\sum_{r=1}^{n}\\left(a_r(x_i) - a_r(x_j)\\right)^2}.$$\n",
    "\n",
    "Um diesen Wert zu berechnen, implementiert die Funktion `euclidean_distance()`. Die Funktion `pairwise_distances()` soll anschließend die paarweisen euklidischen Distanzen einer Punktmenge berechnen und diese als Matrix zurückgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def euclidean_distance(x1, x2):\n",
    "    return NotImplementedError\n",
    "\n",
    "def pairwise_distances(X):\n",
    "    return NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(02.2.2)** Implementierung k-Distance\n",
    "\n",
    "_Für **12** Punkte_\n",
    "\n",
    "Implementiert nun die `k_distance`-Klasse. Orientiert euch hier an den Grundzügen des k-nearest-Neighbour-Algorithmus (wie in Vorlesung und Übung kennengelernt) und passt ihn mit den im Wikipedia-Artikel ([Local Outlier Factor](https://en.wikipedia.org/wiki/Euclidean_distance)) erklärten Formeln an. \n",
    "\n",
    "Die Klasse soll die Methoden `fit()` und `predict()` enthalten. Weitere Hilfsfunktionen (beispielsweise zur Berechnung der Werte des Local Outlier Factors) können die Klasse übersichtlicher machen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class k_distance:\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.x_train = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"Stores training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training samples to be used for LOF computation.\n",
    "        \"\"\"\n",
    "        return NotImplementedError\n",
    "    \n",
    "    def predict(self):\n",
    "        \"\"\"Compute LOF scores for the stored training data.\n",
    "        Expects that fit() has been called before.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        lof : ndarray, shape (n_samples,)\n",
    "            Local Outlier Factor score for each sample.\n",
    "        neigh_idx : ndarray, shape (n_samples, k)\n",
    "            Indices of the k nearest neighbors for each sample.\n",
    "        \"\"\"\n",
    "        return NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test auf Beispieldaten\n",
    "\n",
    "Nun soll euer implementierter Algorithmus angewendet werden! Dafür werden in der folgenden Codezelle ein Trainingsdatensatz `X` mit zwei Clustern und Outliern erzeugt, die von eurer k_distance-Klasse detektiert werden sollten. Anschließend wird das Modell an die Trainingsdaten angepasst und die LOF-Scores erstellt. Wenn euer Algorithmus korrekt implementiert wurde, könnt ihr die Indizes der Outlier in der Ausgabe, sowie im darauffolgenden Plot sehen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "cluster1 = rng.normal(loc=[0, 0], scale=0.5, size=(100, 2))\n",
    "cluster2 = rng.normal(loc=[5, 5], scale=0.5, size=(100, 2))\n",
    "outliers = np.array([[8, 0], [0, 8], [10, 10], [-5, -5], [6, -6]])\n",
    "\n",
    "X = np.vstack([cluster1, cluster2, outliers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "model = k_distance(k=10)\n",
    "# fit model\n",
    "model.fit(X)\n",
    "# predict LOF scores\n",
    "lof_scores_arr, neighbors = model.predict()\n",
    "\n",
    "# print top 10 anomalies\n",
    "order = np.argsort(lof_scores_arr)[::-1]\n",
    "print(\"Top anomalies (index, LOF score):\")\n",
    "for idx in order[:10]:\n",
    "  print(idx, float(lof_scores_arr[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot data and highlight anomalies\n",
    "plt.scatter(X[:,0], X[:,1], s=12, label='data')\n",
    "plt.scatter(X[order[:10],0], X[order[:10],1], c='red', s=40, label='anomalies')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teaching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
